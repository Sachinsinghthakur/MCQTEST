import argparse
from sqlalchemy import  text
from datetime import datetime
from ..utils import get_engine
from ..etl_utils import initialize_spark_session_with_scaling
from pyspark.sql.functions import (col, min as min_, max as max_, 
                                   countDistinct, count, expr, last, avg)

DATETIME_FORMAT = "%Y:%m:%d %H"
PIPELINE_NAME = "test_summary_fact_hourly"


def parse_args():
    parser = argparse.ArgumentParser(description="Run test_summary_fact_hourly ETL")
    parser.add_argument("--env", required=True, help="Database environment")
    parser.add_argument("--backfill_start_time", required=False, help="Backfill start time in 'YYYY:MM:DD HH'")
    parser.add_argument("--backfill_end_time", required=False, help="Backfill end time in 'YYYY:MM:DD HH'")
    return parser.parse_args()

args = parse_args()
env = args.env
start_time = datetime.strptime(args.backfill_start_time, DATETIME_FORMAT) if args.backfill_start_time else None
end_time = datetime.strptime(args.backfill_end_time, DATETIME_FORMAT) if args.backfill_end_time else None

def estimate_input_size(engine, start_time, end_time):
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT COUNT(*) FROM answers
            WHERE timestamp >= :start_time AND timestamp < :end_time
        """), {
            "start_time": start_time,
            "end_time": end_time
        }).fetchone()
        return result[0]


def run_etl():
    engine = get_engine(env)
    cluster_name = 'spark_test_summary_fact_hourly_'+str(env)

    if not start_time or not end_time:
        raise ValueError("Both start_time and end_time must be provided.")

    record_count = estimate_input_size(engine, start_time, end_time)

    spark = initialize_spark_session_with_scaling(cluster_name,record_count)

    jdbc_url = engine.url.render_as_string(hide_password=False)
    df = spark.read \
        .format("jdbc") \
        .option("url", jdbc_url) \
        .option("dbtable", f"""
            (SELECT a.*, q.correct_option, sub.submit_time, sub.completion_status,
                    s.start_time AS session_start
             FROM answers a
             JOIN questions q ON a.question_id = q.question_id
             JOIN sessions s ON a.session_id = s.session_id
             LEFT JOIN submissions sub ON a.student_id = sub.student_id AND a.test_id = sub.test_id
             WHERE a.timestamp >= '{start_time}' AND a.timestamp < '{end_time}'
            ) AS answer_data
        """) \
        .option("user", engine.url.username) \
        .option("password", engine.url.password) \
        .option("driver", "org.postgresql.Driver") \
        .load()

    if df.rdd.isEmpty():
        print("[INFO] No data in time window.")
        return

    df = df.withColumn("is_correct", col("submitted_option") == col("correct_option"))

    agg_df = df.groupBy("student_id", "test_id").agg(
        min_("session_start").alias("test_start_time"),
        max_("submit_time").alias("test_submit_time"),
        last("completion_status").alias("completion_status"),
        countDistinct("session_id").alias("total_sessions"),
        countDistinct("question_id").alias("total_questions"),
        count("question_id").alias("questions_answered"),
        expr("count(question_id) - count(DISTINCT question_id)").alias("questions_revisited"),
        (max_("timestamp").cast("long") - min_("timestamp").cast("long")).alias("total_time_spent"),
        avg(col("is_correct").cast("double")).alias("score")
    )

    agg_df = agg_df.withColumn(
        "avg_time_per_question",
        col("total_time_spent") / col("questions_answered")
    )

    agg_df.write \
        .format("jdbc") \
        .option("url", jdbc_url) \
        .option("dbtable", "test_summary_fact") \
        .option("user", engine.url.username) \
        .option("password", engine.url.password) \
        .option("driver", "org.postgresql.Driver") \
        .mode("append") \
        .save()

    with engine.connect() as conn:
        conn.execute(text("""
            INSERT INTO etl_metadata (pipeline_name, last_run_time, status)
            VALUES (:name, :run_time, :status)
            ON CONFLICT (pipeline_name) DO UPDATE SET last_run_time = :run_time, status = :status
        """), {
            "name": PIPELINE_NAME,
            "run_time": end_time.strftime(DATETIME_FORMAT),
            "status": "success"
        })


if __name__ == "__main__":
    try:
        run_etl()
    except Exception as e:
        print(f"[ERROR] ETL failed: {e}")
        engine = get_engine(env)
        with engine.connect() as conn:
            conn.execute(text("""
                INSERT INTO etl_metadata (pipeline_name, last_run_time, status)
                VALUES (:name, :run_time, :status)
                ON CONFLICT (pipeline_name) DO UPDATE SET last_run_time = :run_time, status = :status
            """), {
                "name": PIPELINE_NAME,
                "run_time": datetime.utcnow().strftime(DATETIME_FORMAT),
                "status": "failed"
            })
        raise
